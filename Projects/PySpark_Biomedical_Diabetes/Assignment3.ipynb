{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e3b136-33d7-487b-a1bb-b2620e37cdd6",
   "metadata": {},
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7ce5b-0b29-429a-b196-714cce52392a",
   "metadata": {},
   "source": [
    "Step 1 (50 points): Order the pair of strings alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40bb2486-255f-4a18-bc4a-1622632e1c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 10:54:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/25 10:54:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/25 10:54:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.38:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c64b09f-4ab2-4952-bada-117a71d65ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from rel.csv into RDD\n",
    "rel_rdd = sc.textFile(\"rel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ddadf3-40c2-4409-acfc-2ee997fa866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C0005778', 'C0005790'),\n",
       " ('C1255279', 'C3537249'),\n",
       " ('C0002520', 'C1255446'),\n",
       " ('C0596019', 'C1255552'),\n",
       " ('C0004611', 'C1254417')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean and order the pair of strings alphabetically\n",
    "ordered_pairs_rdd = rel_rdd.map(lambda line: tuple(sorted(line.replace('\"', '').split(','))))\n",
    "ordered_pairs_rdd.take(5) # Preview first 5 ordered pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73798149-01dd-497c-adc1-16b16cff3433",
   "metadata": {},
   "source": [
    "Step 2 (40 points): Count the number of instances for each ordered pair resulted in Step 1. Output the result in a plain text file (named as “pair-count.txt”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e9547b9-89cc-4a32-bf39-cc3cfe50f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/25 10:55:06 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count number of instances for each ordered pair\n",
    "pair_counts_rdd = ordered_pairs_rdd.map(lambda pair: (pair, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Format output\n",
    "formatted_rdd = pair_counts_rdd.map(lambda x: f'\"{x[0][0]}\",\"{x[0][1]}\" {x[1]}')\n",
    "\n",
    "# Output the result\n",
    "formatted_rdd.coalesce(1).saveAsTextFile(\"pair-count-tmp\") # Save to a temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca80e8b6-9336-4a11-8d8c-ad64177f55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Rename part file to final output\n",
    "for file in os.listdir(\"pair-count-tmp\"):\n",
    "    if file.startswith(\"part-\"):\n",
    "        shutil.move(f\"pair-count-tmp/{file}\", \"pair-count.txt\")\n",
    "        break\n",
    "\n",
    "# Remove the temporary folder\n",
    "shutil.rmtree(\"pair-count-tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1a2fc-0351-416d-93cd-dc3c0f4ef9fb",
   "metadata": {},
   "source": [
    "Step 3 (10 points): Count the total number of ordered pairs, that is, how many unique ordered pairs are obtained (rather than the total number of instances of ordered pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2344aaea-80ff-4036-90fb-4e48b80c3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================================>           (32 + 8) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique ordered pairs: 12946540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count unique ordered pairs\n",
    "unique_pair_count = pair_counts_rdd.count()\n",
    "print(f\"Total unique ordered pairs: {unique_pair_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439de5d-4eb2-480d-a158-f232261ac6f3",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6749db-e8d2-41e3-b32e-1071683d92f4",
   "metadata": {},
   "source": [
    "Step 1 (20 points): Remove the rows where the column “BloodPressure”, “BMI” or “Glucose” is zero (note: remove the row as long as one of the three columns is zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22a06b66-b8c9-4392-a1ff-d86acc3e58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53bfd43d-4b04-4f6c-9600-da91a97fe241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes.csv into DataFrame\n",
    "df = spark.read.csv(\"diabetes.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10efea5b-8329-4350-b885-6040e7bf1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows\n",
    "filtered_df = df.filter((df.BloodPressure != 0) & (df.BMI != 0) & (df.Glucose != 0))\n",
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02717c-a259-4c4f-bebc-a5adb82f2441",
   "metadata": {},
   "source": [
    "Step 2 (10 points): Convert the categorical column “Pregnancies” into one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf518cd7-1888-4ab9-972d-01da9c42bcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+--------------+\n",
      "|Pregnancies|PregnanciesIndex|PregnanciesVec|\n",
      "+-----------+----------------+--------------+\n",
      "|6          |6.0             |(16,[6],[1.0])|\n",
      "|1          |0.0             |(16,[0],[1.0])|\n",
      "|8          |8.0             |(16,[8],[1.0])|\n",
      "|1          |0.0             |(16,[0],[1.0])|\n",
      "|0          |1.0             |(16,[1],[1.0])|\n",
      "+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode \"Pregnancies\"\n",
    "indexer = StringIndexer(inputCol=\"Pregnancies\", outputCol=\"PregnanciesIndex\")\n",
    "indexed_df = indexer.fit(filtered_df).transform(filtered_df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"PregnanciesIndex\", outputCol=\"PregnanciesVec\")\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "encoded_df.select(\"Pregnancies\", \"PregnanciesIndex\", \"PregnanciesVec\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8217eb-cb92-4e7f-bcef-9f97d83bfaba",
   "metadata": {},
   "source": [
    "Step 3 (10 points): Create a single column with all the features collated together using VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa25ed39-b00c-42d5-a20f-a631aa660ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+-------+\n",
      "|features                                                                 |Outcome|\n",
      "+-------------------------------------------------------------------------+-------+\n",
      "|(23,[6,16,17,18,20,21,22],[1.0,148.0,72.0,35.0,33.6,0.627,50.0])         |1      |\n",
      "|(23,[0,16,17,18,20,21,22],[1.0,85.0,66.0,29.0,26.6,0.351,31.0])          |0      |\n",
      "|(23,[8,16,17,20,21,22],[1.0,183.0,64.0,23.3,0.672,32.0])                 |1      |\n",
      "|(23,[0,16,17,18,19,20,21,22],[1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0])  |0      |\n",
      "|(23,[1,16,17,18,19,20,21,22],[1.0,137.0,40.0,35.0,168.0,43.1,2.288,33.0])|1      |\n",
      "+-------------------------------------------------------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collate features in a single column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"PregnanciesVec\", \"Glucose\", \"BloodPressure\", \"SkinThickness\",\n",
    "               \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "final_df = assembler.transform(encoded_df)\n",
    "final_df.select(\"features\", \"Outcome\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22105e9c-a9ec-4b83-964d-5e3942236bd3",
   "metadata": {},
   "source": [
    "Step 4 (10 points): Random split the collated data into training (70%) and testing (30%) datasets and use 2017 as the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dab1b234-5712-43e1-bf4d-0a8e6354f137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 512\n",
      "Test set size: 212\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_df, test_df = final_df.randomSplit([0.7, 0.3], seed=2017)\n",
    "\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b84f8-d9c9-4301-8972-eba18af24235",
   "metadata": {},
   "source": [
    "Step 5 (40 points): Implement a random forest classifier and specify the number of decision trees as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50d80afe-a399-49ea-b874-d15eee987b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-------+----------+----------------------------------------+\n",
      "|features                                                                |Outcome|prediction|probability                             |\n",
      "+------------------------------------------------------------------------+-------+----------+----------------------------------------+\n",
      "|(23,[1,16,17,20,21,22],[1.0,57.0,60.0,21.7,0.735,67.0])                 |0      |0.0       |[0.8051880801040465,0.19481191989595348]|\n",
      "|(23,[1,16,17,20,21,22],[1.0,67.0,76.0,45.3,0.194,46.0])                 |0      |0.0       |[0.7304940685195123,0.2695059314804877] |\n",
      "|(23,[1,16,17,18,19,20,21,22],[1.0,84.0,64.0,22.0,66.0,35.8,0.545,21.0]) |0      |0.0       |[0.8084763801181907,0.19152361988180933]|\n",
      "|(23,[1,16,17,18,19,20,21,22],[1.0,93.0,60.0,25.0,92.0,28.7,0.532,22.0]) |0      |0.0       |[0.837044023188082,0.16295597681191798] |\n",
      "|(23,[1,16,17,18,19,20,21,22],[1.0,93.0,100.0,39.0,72.0,43.4,1.021,35.0])|0      |0.0       |[0.6122017699870439,0.38779823001295616]|\n",
      "+------------------------------------------------------------------------+-------+----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"Outcome\", featuresCol=\"features\", numTrees=20)\n",
    "rf_model = rf.fit(train_df)\n",
    "predictions = rf_model.transform(test_df)\n",
    "predictions.select(\"features\", \"Outcome\", \"prediction\", \"probability\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439fa27-f12e-47b4-b67f-02fd41633b39",
   "metadata": {},
   "source": [
    "Step 6 (10 points): Evaluate the performance of the random forest classifier using the ROC curve metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c4635a3-6210-4a2f-90ab-1b570b9e9993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.8446731857951623\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using ROC AUC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC AUC: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
